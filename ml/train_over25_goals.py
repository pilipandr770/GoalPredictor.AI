"""
–¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—É Over/Under 2.5 –≥–æ–ª—ñ–≤
–ê–∫—Ü–µ–Ω—Ç –Ω–∞ –Ü–°–¢–û–†–Ü–Æ –ì–û–õ–Ü–í —è–∫ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—É —Ñ—ñ—á—É
"""
import os
import sys
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler
import joblib
import warnings
warnings.filterwarnings("ignore")


def load_data():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ –¥–∞–Ω—ñ"""
    print("="*70)
    print("üéØ –¢–†–ï–ù–£–í–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü: Over/Under 2.5 –≥–æ–ª—ñ–≤")
    print("="*70)
    
    # –®—É–∫–∞—î–º–æ —Ñ–∞–π–ª –∑ –¥–∞–Ω–∏–º–∏
    data_paths = [
        'ml/data/training_data_enhanced.csv',
        'ml/data/training_data.csv',
        'ml/data/prepared_data.csv',
        'ml/data/enhanced_data.csv',
        'data/prepared_matches.csv'
    ]
    
    df = None
    for path in data_paths:
        if os.path.exists(path):
            print(f"\nüìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {path}")
            df = pd.read_csv(path)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –º–∞—Ç—á—ñ–≤")
            break
    
    if df is None:
        raise FileNotFoundError("‚ùå –§–∞–π–ª –∑ –¥–∞–Ω–∏–º–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –ó–∞–ø—É—Å—Ç—ñ—Ç—å ml/download_data.py")
    
    return df


def create_goal_focused_features(df):
    """
    –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ñ—ñ—á—ñ –∑ –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –ì–û–õ–ò
    –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ —Ñ—ñ—á—ñ - —ñ—Å—Ç–æ—Ä—ñ—è –∑–∞–±–∏—Ç–∏—Ö –≥–æ–ª—ñ–≤
    """
    print("\nüîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ—ñ—á—ñ–≤ –∑ –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –≥–æ–ª–∏...")
    
    feature_columns = []
    
    # ============================================
    # üéØ –ù–ê–ô–í–ê–ñ–õ–ò–í–Ü–®–Ü –§–Ü–ß–Ü: –Ü—Å—Ç–æ—Ä—ñ—è –≥–æ–ª—ñ–≤
    # ============================================
    
    # –ì–æ–ª–∏ –≥–æ—Å–ø–æ–¥–∞—Ä—ñ–≤ (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ!)
    goal_features_home = [
        'home_recent_goals_for',      # –ó–∞–±–∏—Ç—ñ –≥–æ–ª–∏
        'home_recent_goals_against',  # –ü—Ä–æ–ø—É—â–µ–Ω—ñ –≥–æ–ª–∏
        'home_avg_goals',              # –°–µ—Ä–µ–¥–Ω—ñ –≥–æ–ª–∏ –∑–∞ —Å–µ–∑–æ–Ω
        'home_scoring_trend'           # –¢—Ä–µ–Ω–¥ –∑–∞–±–∏–≤–∞–Ω–Ω—è –≥–æ–ª—ñ–≤
    ]
    
    # –ì–æ–ª–∏ –≥–æ—Å—Ç–µ–π (–Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ!)
    goal_features_away = [
        'away_recent_goals_for',      # –ó–∞–±–∏—Ç—ñ –≥–æ–ª–∏
        'away_recent_goals_against',  # –ü—Ä–æ–ø—É—â–µ–Ω—ñ –≥–æ–ª–∏
        'away_avg_goals',              # –°–µ—Ä–µ–¥–Ω—ñ –≥–æ–ª–∏ –∑–∞ —Å–µ–∑–æ–Ω
        'away_scoring_trend'           # –¢—Ä–µ–Ω–¥ –∑–∞–±–∏–≤–∞–Ω–Ω—è –≥–æ–ª—ñ–≤
    ]
    
    # –ó–∞–≥–∞–ª—å–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –≥–æ–ª—ñ–≤
    goal_features_general = [
        # –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 'total_goals' - —Ü–µ DATA LEAKAGE!
        'expected_total_goals'         # –û—á—ñ–∫—É–≤–∞–Ω—ñ –≥–æ–ª–∏
    ]
    
    # –û–±'—î–¥–Ω–∞—Ç–∏ –≤—Å—ñ —Ñ—ñ—á—ñ –≥–æ–ª—ñ–≤
    all_goal_features = goal_features_home + goal_features_away + goal_features_general
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —è–∫—ñ —Ñ—ñ—á—ñ —î –≤ –¥–∞–Ω–∏—Ö
    available_features = []
    for feat in all_goal_features:
        if feat in df.columns:
            available_features.append(feat)
    
    print(f"   ‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(available_features)} —Ñ—ñ—á—ñ–≤ –≥–æ–ª—ñ–≤:")
    for feat in available_features:
        print(f"      ‚Ä¢ {feat}")
    
    feature_columns.extend(available_features)
    
    # ============================================
    # üìä –î–û–î–ê–¢–ö–û–í–Ü –§–Ü–ß–Ü (–º–µ–Ω—à–∏–π –≤–µ—Å)
    # ============================================
    
    # –§–æ—Ä–º–∞ –∫–æ–º–∞–Ω–¥ (–¥–æ–ø–æ–º—ñ–∂–Ω—ñ —Ñ—ñ—á—ñ)
    form_features = [
        'home_recent_form_points',
        'away_recent_form_points',
        'home_recent_wins',
        'away_recent_wins',
        'home_total_matches',
        'away_total_matches'
    ]
    
    # Head-to-head —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    h2h_features = [
        'h2h_home_wins',
        'h2h_draws',
        'h2h_away_wins'
    ]
    
    # –î–æ–¥–∞—Ç–∏ —è–∫—â–æ —î
    for feat in form_features + h2h_features:
        if feat in df.columns and feat not in feature_columns:
            feature_columns.append(feat)
    
    print(f"\nüìã –í—Å—å–æ–≥–æ —Ñ—ñ—á—ñ–≤: {len(feature_columns)}")
    
    return feature_columns


def prepare_data(df):
    """–ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"""
    print("\nüìä –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö...")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É: Over 2.5 –≥–æ–ª—ñ–≤
    if 'over_2_5' not in df.columns:
        if 'total_goals' in df.columns:
            df['over_2_5'] = (df['total_goals'] > 2.5).astype(int)
        elif 'home_score' in df.columns and 'away_score' in df.columns:
            df['over_2_5'] = ((df['home_score'] + df['away_score']) > 2.5).astype(int)
        else:
            raise ValueError("‚ùå –ù–µ–º–æ–∂–ª–∏–≤–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É over_2_5")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
    over_25_rate = df['over_2_5'].mean()
    print(f"\nüéØ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Over 2.5:")
    print(f"   Over 2.5: {df['over_2_5'].sum()} –º–∞—Ç—á—ñ–≤ ({over_25_rate:.1%})")
    print(f"   Under 2.5: {(1-df['over_2_5']).sum()} –º–∞—Ç—á—ñ–≤ ({1-over_25_rate:.1%})")
    
    # –í–∏–±—Ä–∞—Ç–∏ —Ñ—ñ—á—ñ –∑ –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –≥–æ–ª–∏
    feature_columns = create_goal_focused_features(df)
    
    # –í–∏–¥–∞–ª–∏—Ç–∏ —Ä—è–¥–∫–∏ –∑ –ø—Ä–æ–ø—É—â–µ–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º–∏
    df_clean = df[feature_columns + ['over_2_5']].dropna()
    print(f"\n‚úÖ –ü—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è: {len(df_clean)} –º–∞—Ç—á—ñ–≤")
    
    X = df_clean[feature_columns]
    y = df_clean['over_2_5']
    
    return X, y, feature_columns


def train_model(X, y):
    """–ù–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å –∑ –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å"""
    print("\nü§ñ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    print("="*70)
    
    # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –¥–∞–Ω—ñ
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìä Train: {len(X_train)}, Test: {len(X_test)}")
    print(f"   Train Over 2.5: {y_train.mean():.1%}")
    print(f"   Test Over 2.5: {y_test.mean():.1%}")
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü—ñ—è (–¥–æ–ø–æ–º–æ–∂–µ –∑ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é —Ñ—ñ—á—ñ–≤)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ –º–æ–¥–µ–ª–µ–π
    models = {
        'RandomForest': RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        ),
        'GradientBoosting': GradientBoostingClassifier(
            n_estimators=200,
            max_depth=7,
            learning_rate=0.1,
            random_state=42
        )
    }
    
    best_model = None
    best_model_name = None
    best_score = 0
    
    print("\nüéØ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π:")
    for name, model in models.items():
        print(f"\n   {name}:")
        
        # –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
        if name == 'RandomForest':
            model.fit(X_train_scaled, y_train)
        else:
            model.fit(X_train_scaled, y_train)
        
        # –û—Ü—ñ–Ω–∫–∞
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)
        y_proba_test = model.predict_proba(X_test_scaled)[:, 1]
        
        train_acc = accuracy_score(y_train, y_pred_train)
        test_acc = accuracy_score(y_test, y_pred_test)
        test_auc = roc_auc_score(y_test, y_proba_test)
        
        # Cross-validation
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
        cv_mean = cv_scores.mean()
        
        print(f"      Train Accuracy: {train_acc:.2%}")
        print(f"      Test Accuracy:  {test_acc:.2%}")
        print(f"      Test AUC:       {test_auc:.3f}")
        print(f"      CV Mean:        {cv_mean:.2%} (+/- {cv_scores.std():.2%})")
        
        if cv_mean > best_score:
            best_score = cv_mean
            best_model = model
            best_model_name = name
    
    print(f"\n‚úÖ –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å: {best_model_name} (CV: {best_score:.2%})")
    
    # –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
    print("\n" + "="*70)
    print(f"üìä –î–ï–¢–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢: {best_model_name}")
    print("="*70)
    
    y_pred_final = best_model.predict(X_test_scaled)
    y_proba_final = best_model.predict_proba(X_test_scaled)[:, 1]
    
    print("\nüìà Classification Report:")
    print(classification_report(y_test, y_pred_final, 
                                target_names=['Under 2.5', 'Over 2.5'],
                                zero_division=0))
    
    print("\nüìä Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred_final)
    print(cm)
    print(f"\n   True Negatives:  {cm[0,0]} (–ø—Ä–∞–≤–∏–ª—å–Ω–æ Under 2.5)")
    print(f"   False Positives: {cm[0,1]} (–ø–æ–º–∏–ª–∫–æ–≤–æ Over 2.5)")
    print(f"   False Negatives: {cm[1,0]} (–ø–æ–º–∏–ª–∫–æ–≤–æ Under 2.5)")
    print(f"   True Positives:  {cm[1,1]} (–ø—Ä–∞–≤–∏–ª—å–Ω–æ Over 2.5)")
    
    # –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ñ—ñ—á—ñ–≤
    if hasattr(best_model, 'feature_importances_'):
        print("\nüéØ –¢–û–ü-10 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö —Ñ—ñ—á—ñ–≤:")
        importances = best_model.feature_importances_
        indices = np.argsort(importances)[::-1][:10]
        
        for i, idx in enumerate(indices, 1):
            print(f"   {i}. {X.columns[idx]}: {importances[idx]:.4f}")
    
    return best_model, scaler, best_model_name


def save_model(model, scaler, feature_columns, model_name):
    """–ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å"""
    print("\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    
    models_dir = 'ml/models'
    os.makedirs(models_dir, exist_ok=True)
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å
    model_path = os.path.join(models_dir, 'over_2_5_goals_model.pkl')
    joblib.dump(model, model_path)
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å: {model_path}")
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ scaler
    scaler_path = os.path.join(models_dir, 'over_2_5_scaler.pkl')
    joblib.dump(scaler, scaler_path)
    print(f"   ‚úÖ Scaler: {scaler_path}")
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ñ—ñ—á—ñ–≤
    features_path = os.path.join(models_dir, 'over_2_5_features.pkl')
    joblib.dump(feature_columns, features_path)
    print(f"   ‚úÖ Features: {features_path}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ
    metadata = {
        'model_type': model_name,
        'num_features': len(feature_columns),
        'features': list(feature_columns),
        'target': 'over_2_5',
        'version': '1.0'
    }
    
    metadata_path = os.path.join(models_dir, 'over_2_5_metadata.pkl')
    joblib.dump(metadata, metadata_path)
    print(f"   ‚úÖ Metadata: {metadata_path}")
    
    print("\nüéâ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ!")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    try:
        # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ
        df = load_data()
        
        # 2. –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ
        X, y, feature_columns = prepare_data(df)
        
        # 3. –ù–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
        model, scaler, model_name = train_model(X, y)
        
        # 4. –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å
        save_model(model, scaler, feature_columns, model_name)
        
        print("\n" + "="*70)
        print("‚úÖ –¢–†–ï–ù–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*70)
        print("\nüìù –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:")
        print("   1. –ú–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑—É—î —á–∏ –±—É–¥–µ Over/Under 2.5 –≥–æ–ª—ñ–≤")
        print("   2. –û—Å–Ω–æ–≤–Ω–∞ —Ñ—ñ—á–∞: —ñ—Å—Ç–æ—Ä—ñ—è –≥–æ–ª—ñ–≤ –∫–æ–º–∞–Ω–¥")
        print("   3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π services/prediction_service.py –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤")
        
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
